---
title: "High Dimensionality Dataset Reduction Methodologies in Applied Machine Learning"
excerpt: "**Data Science and Data Analytics: Opportunities and Challenges 2020** - Taylor and Francis Book Chapter Publication (Routledge) CRC Press"
toc: true
toc_sticky: true

categories:
  - research

tags:
  - Dimensionality Reduction
  - PCA
  - Covariance Matrix
  - t-SNE
  - Support Vector Machines
  - k-Nearest Neighbours

last_modified_at: 2020-10-23T08:06:00-05:00

header:
  teaser: /assets/images/DIM_REDN/DIM_RED_VIZ.png
  overlay_image: /assets/images/DIM_REDN/DIM_RED_VIZ.png
  overlay_filter: 0.5 # same as adding an opacity of 0.5 to a black background
  caption: "Image credit: [**Jackson Wu, Medium**](https://medium.com/@jwu2/improving-collaborative-filtering-with-dimensionality-reduction-a99d08585dab)"
  actions:
    - label: "View Book"
      url: "https://www.routledge.com/Data-Science-and-Data-Analytics-Opportunities-and-Challenges/Tyagi/p/book/9780367628826"
    - label: "View Code on Github"
      url: "https://github.com/khanfarhan10/DIMENSIONALITY_REDUCTION"
use_math: true
mathjax: true
---

```python
from math import Dimensionality_Reduction
```

# What's inside this Book Chapter :

This blog is a detailed , yet lucid overview of the book chapter , **"High Dimensionality Dataset Reduction Methodologies in Applied Machine Learning"** from the **"Taylor and Francis Book Publication (Routledge)"** for the book **"Data Science and Data Analytics: Opportunities and Challenges 2020"**.

# Read the Chapters in Detail :

Yet to come post publication of book chapter.


# Table of Contents:

### 7.1 Problems Faced with High Dimensionality Data: An Introduction - 98  
### 7.2 Dimensionality Reduction Algorithms with Visualizations - 99  
#### 7.2.1 Feature Selection Using Covariance Matrix - 99  
7.2.1.1 Importing the Modules - 99  
7.2.1.2 The Boston Housing Dataset - 100  
7.2.1.3 Perform Basic Data Visualization - 101  
7.2.1.4 Pearson Coefficient Correlation Matrix - 103  
7.2.1.5 Detailed Correlation Matrix Analysis - 103  
7.2.1.6 3-Dimensional Data Visualization - 106  
7.2.1.7 Extracting the Features and Target - 108  
7.2.1.8 Feature Scaling - 109  
7.2.1.9 Create Training and Testing Datasets - 109  
7.2.1.10 Training and Evaluating Regression Model with Reduced Dataset - 109  
7.2.1.11 Limitations of the Correlation Matrix Analysis - 111  
#### 7.2.2 t-Distributed Stochastic Neighbor Embedding (t-SNE) - 111  
7.2.1.1 The MNIST Handwritten Digits Dataset - 111  
7.2.1.2 Perform Exploratory Data Visualization - 112  
7.2.1.3 Random Sampling of the Large Dataset - 112  
7.2.1.4 T-Distributed Stochastic Neighboring Entities (t-SNE) – An Introduction - 112  
7.2.1.5 Probability and Mathematics Behind t-SNE - 114  
7.2.1.6 Implementing and Visualizing t-SNE in 2-D - 115  
7.2.1.7 Implementing adn Visualizing t-SNE in 3-D - 116  
7.2.1.8 Applying k-Nearest Neighbors (k-NN) on the t-SNE MNIST Dataset - 118  
7.2.1.9 Data Preparation – Extracting the Features and Target - 118  
7.2.1.10 Create Training and Testing Dataset - 118  
7.2.1.11 Choosing the k-NN hyperparameter – k - 119  
7.2.1.12 Model Evaluation – Jaccard Index, F1 Score, Model Accuracy, and Confusion Matrix - 121  
7.2.1.13 Limitations of the t-SNE Algorithm - 123  
#### 7.2.2 Principle Component Analysis (PCA) - 124  
7.2.1.1 The UCI Breast Cancer Dataset - 124  
7.2.2.2 Perform Basic Data Visualization - 125  
7.2.2.3 Create Training and Testing Dataset - 128  
7.2.2.4 Principal Component Analysis (PCA): An Introduction - 128  
7.2.2.5 Transposing the Data for Usage into Python - 129  
7.2.2.6 Standardization – Finding the Mean Vector - 129  
7.2.2.7 Computing the n-Dimensional Covariance Matrix - 129  
7.2.2.8 Calculating the Eigenvalues and Eigenvectors of the Covariance Matrix - 130  
7.2.2.9 Sorting the Eigenvalues and Corresponding Eigenvectors Obtained - 131  
7.2.2.10 Construct Feature Matrix – Choosing the k Eigenvectors with the Largest Eigenvalues - 131  
7.2.2.11 Data Transformation – Derivation of New Dataset by PCA – Reduced Number of Dimensions - 131  
7.2.2.12 PCA Using Scikit-Learn - 133  
7.2.2.13 Verification of Library and Stepwise PCA - 133  
7.2.3.14 PCA – Captured Variance and Data Lost - 134  
7.2.3.15 PCA Visualizations - 134  
7.2.3.16 Splitting the Data into Test and Train Sets - 135  
7.2.3.17 An Introduction to Classification Modeling with Support Vector Machines (SVM) - 137  
7.2.3.18 Types of SVM - 138  
7.2.3.19 Limitations of PCA - 142  
7.2.3.20 PCA vs. t-SNE - 143  
### Conclusion - 143 


