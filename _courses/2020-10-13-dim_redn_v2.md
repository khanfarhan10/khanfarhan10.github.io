---
title: "High Dimensionality Dataset Reduction Methodologies in Applied Machine Learning"
author_profile: false
layout: single
classes: wide
toc: false
toc_sticky: true
tags:
  - Dimensionality Reduction Methodologies
  - Machine Learning
  - High Dimensionality Data Handling
date: 2016-11-14 08:15:00 +0530
categories: courses
header:
  teaser: "/assets/images/DIM_REDN/DIM_RED_VIZ.png"
excerpt: "Taylor and Francis Book Publication (Routledge) : DIMENSIONALITY REDUCTION ALGORITHMS IN APPLIED MACHINE LEARNING"
description: "Taylor and Francis Book Publication (Routledge) : DIMENSIONALITY REDUCTION ALGORITHMS IN APPLIED MACHINE LEARNING"
og_image: "/assets/images/DIM_REDN/DIM_RED_VIZ.png"
use_math: true
mathjax: true
sidebar:
  - title: "High Dimensionality Dataset Reduction Methodologies in Applied Machine Learning"
    image: "/assets/images/DIM_REDN/DIM_RED_VIZ.png"
    image_alt: "Dimensionality Projection"
    text: "Taylor and Francis Book Publication (Routledge) : DIMENSIONALITY REDUCTION ALGORITHMS IN APPLIED MACHINE LEARNING"
    nav: DSDA_docs
---

<!--
layout: single
classes : wide
-->

**Farhan Hai Khan<sup>a</sup>, Tannistha Pal<sup>b</sup>**
{: .text-center}
**a. Department of Electrical Engineering, Institute of Engineering & Management, Kolkata, India, njrfarhandasilva10@gmail.com**
{: .text-center}
**b. Department of Electronics and Communication Engineering, Institute of Engineering & Management, Kolkata, India, paltannistha@gmail.com**
{: .text-center}

<!--
Useful Minimal Mistakes Theme Links:

https://mmistakes.github.io/minimal-mistakes/docs/utility-classes/

https://mmistakes.github.io/minimal-mistakes/docs/helpers/#gallery

https://mmistakes.github.io/minimal-mistakes/docs/layouts/

-->

## **Abstract**

A common problem faced while handling multi-featured datasets is the high amount of dimensionality that they often consist of, leading to barriers in generalized hands-on Machine Learning. These datasets also give a drastic impact on the performance of Machine Learning algorithms, being memory inefficient and frequently leading to model overfitting. It often becomes difficult to visualize or gain insightful knowledge on the data features such as presence of outliers.

This chapter will help data analysts reduce data dimensionality using various methodologies such as:<br>

1. Feature Selection using Covariance Matrix
2. Principal Component Analysis (PCA)
3. t-distributed Stochastic Neighbour Embedding (t-SNE)

Under applications of Dimensionality Reduction Algorithms with Visualizations, firstly, we introduce the Boston Housing Dataset and use the Correlation Matrix to apply Feature Selection on the strongly positive correlated data and perform Simple Linear Regression over the new features.Then we use UCI Breast Cancer Dataset to perform PCA Analysis with Support Vector Machine Classification (SVM). Lastly, we apply t-SNE to MNIST Handwritten Digits Dataset and use k-Nearest Neighbours (kNNs) clustering for classification.

Finally, we explore the benefits of using Dimensionality Reduction Methods and provide a comprehensive overview of reduction in storage space, efficient models,feature selection guidelines ,redundant data removal and outlier analysis.

**_Keywords : Dimensionality Reduction, Feature Selection, Covariance Matrix, PCA , t-SNE_**

## **Table of Contents**

1. Problems faced with Multi-Dimensional Datasets
   1. Data Intuition
   2. Data Visualization Constraints
   3. Outlier Detection
2. Dimensionality Reduction Algorithms with Visualizations
   1. Feature Selection using Covariance Matrix
   2. Principal Component Analysis (PCA)
   3. t-distributed Stochastic Neighbour Embedding (t-SNE)
3. Benefits of Dimensionality Reduction
   1. Storage Space Reduction
   2. Computation Time Optimization
   3. Redundant Feature Removal
   4. Incorrect Data Removal

## 1. Problems faced with High Dimensionality Data : An Introduction

<!--References
https://thenewstack.io/3-new-techniques-for-data-dimensionality-reduction-in-machine-learning/

https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/#:~:text=Dimensionality%20reduction%20refers%20to%20techniques%20for%20reducing%20the%20number%20of,%E2%80%9Cessence%E2%80%9D%20of%20the%20data.

https://medium.com/@cxu24/why-dimensionality-reduction-is-important-dd60b5611543
-->

<!--
### 1.1 **_An Introduction on High Dimensional Data_**
-->

<!--
<i><blockquote> "...dimensionality reduction yields a more compact, more easily interpretable representation of the target concept, focusing the userâ€™s attention on the most relevant variables."</blockquote></i>

-Page 289, Data Mining: Practical Machine Learning Tools and Techniques, 4th edition, 2016.

https://amzn.to/2tlRP9V
-->

<!--
<p align='right'> Your Text </p>
<p style='text-align: right;'> Your Text </p>
<style>body {text-align: right}</style>
<div style="text-align: right"> your-text-here </div>
<p style="text-align:right">This is some text in a paragraph.</p>
-->

<blockquote> "Dimensionality Reduction leads to a comprehensive, precise & compressed depiction of the target output variables, by reducing redundant input variables." 
<p align='right'><b>- Farhan Khan & Tannistha Pal.</b></p>
</blockquote>

_In the field of artificial intelligence, data explosion has created a plethora of input data & features to be fed into machine learning algorithms. Since most of the real-world data is multi-dimensional in nature, data scientists & data analysts require the core concepts of dimensionality reduction mechanisms for better :_<br/><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_(i) Data Intuition : Visualization, Outlier Detection & Noise Reduction_<br/><br/>
&nbsp;&nbsp;&nbsp;&nbsp;_(ii) Performance Efficiency : Faster Training Intervals & Reduced Computational Processing Time_<br/><br/>
&nbsp;&nbsp;&nbsp;&nbsp;_(iii) Generalization : Prevents Overfitting (High Variance & Low Bias)_<br/><br/>
_This chapter introduces the practical working implementation of these reduction algorithms in applied machine learning._

<!--
<p>Anything you'd like to mention goes here: <blockquote>"Insert actual quote here."</blockquote> - Mr. Name</p>
-->
<!--4 nbsp - non breaking space is a tab &nbsp;&nbsp;&nbsp;&nbsp;-->

<i>Multiple features make it difficult to obtain valuable insights into data, as the visualization plots obtained can be 3-Dimensional at most. Due to this limitation, dependent properties/operations such as Outlier Detection and Noise Removal become more and more non-intuitive to perform. Applying dimensionality reduction helps in identifying these properties more nonchalantly.</i>

<i>Due to this reduced/compressed form of data, faster mathematical operations such as Scaling, Categorization etc. can be performed. Also, the data is more cleansed and this further solves the issues of overfitting a model.</i>

<i>
Dimensionality Reduction can be broadly classified into :<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(i) Feature Selection Techniques : Feature Selection attempts to train the machine learning model by selectively choosing a subset of the original feature set based on some criteria. Hence, redundant and obsolete characteristics could be eliminated without much information loss. Examples - Correlation Matrix Thresholding & Chi Squared Test Selection.<br/>
&nbsp;&nbsp;&nbsp;&nbsp;(ii) Feature Extraction/Projection Techniques : This method projects the original input features from the high dimensional space by summarizing most statistics and removing redundant data / manipulating to create new relevant output features with reduced dimensionality (fewer dimensional space). Examples - Principle Component Analysis (PCA) , Linear Discriminant Analysis (LDA), t-distributed Stochastic Neighbour Embedding(t-SNE) & Isometric Mapping (IsoMap).<br/><br/></i>

_However, we have limited our discussion to Correlation Matrices, PCA & t-SNE only, as covering all such techniques is beyond the scope of this book/chapter._

<!--
TODO

3+ dims hard to visualize
outlier noise follow
 del unnecessary rows
Std scaler less time, lesss size,

Quick Tip
SEEDS
-->

## 2. Dimensionality Reduction Algorithms with Visualizations

### 2.1 **_Feature Selection using Covariance Matrix_**

**Objective :** Introduce Boston Housing Dataset and use the obtained Correlation Matrix to apply Feature Selection on the strongly positive correlated data and perform Regression over the selective features.

<!-- References
https://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155

https://www.geeksforgeeks.org/ml-boston-housing-kaggle-challenge-with-linear-regression/

https://towardsdatascience.com/polynomial-regression-bbe8b9d97491

Ridge & Lasso ??? -->

<!--Alert message.-->

We will need 3 datasets for this chapter, each of which have been documented on our [github repository](https://github.com/khanfarhan10/DIMENSIONALITY_REDUCTION).
Hence we will create a local copy (clone) of that repo here.

<!--You can read more about git and github here.??-->

```python
!git clone https://github.com/khanfarhan10/DIMENSIONALITY_REDUCTION.git
```

    Cloning into 'DIMENSIONALITY_REDUCTION'...
    remote: Enumerating objects: 14, done.[K
    remote: Counting objects: 100% (14/14), done.[K
    remote: Compressing objects: 100% (12/12), done.[K
    remote: Total 14 (delta 1), reused 0 (delta 0), pack-reused 0[K
    Unpacking objects: 100% (14/14), done.

Firstly we will import all the necessary libraries that we will be requiring for Dataset Reductions.

```python
import numpy as np               # Mathematical Functions , Linear Algebra, Matrix Operations
import pandas as pd              # Data Manipulations,  Data Analysis/Storing/Preparation
import matplotlib.pyplot as plt  # Simple Data Visualization , Basic Plotting Utilities
plt.style.use("dark_background") #just a preference of the authors, adds visual attractiveness
import seaborn as sns            # Advanced Data Visualization, High Level Figures Interfacing
%matplotlib inline
# used for Jupyter Notebook Plotting
#%matplotlib notebook            # This can be used as an alternative as the plots obtained will be interactive in nature.
```

<!--info-->
<!--Alert Box-->

{% capture notice-text1 %}
In Applied Machine Learning, it is essential to make experiments reproducable and at the same time keeping weights as completely random. The Seed of a Pseudo Random Number Generator (PRNG) acheives the exact same task by initializing values with the same conditions everytime a program is executed. We have used a constant value (universal seed) of 42 throughout the course of this chapter.

More info on [PRNGs](https://www.geeksforgeeks.org/pseudo-random-number-generator-prng/) and [Seeds](https://numpy.org/doc/stable/reference/random/generator.html).
{% endcapture %}

<div class="notice--primary">
  <h4 class="no_toc">Initial Pseudo Random Number Generator Seeds:</h4>
  {{ notice-text1 | markdownify }}
</div>

<!-- original text
**Initial Pseudo Random Number Generator Seeds**

In Applied Machine Learning, it is essential to make experiments reproducable and at the same time keeping weights as completely random. The Seed of a Pseudo Random Number Generator (PRNG) acheives the exact same task by initializing values with the same conditions everytime a program is executed. We have used a constant value (universal seed) of 42 throughout the course of this chapter.

More info on [PRNGs](https://www.geeksforgeeks.org/pseudo-random-number-generator-prng/) and [Seeds](https://numpy.org/doc/stable/reference/random/generator.html).
{: .notice--primary}
-->

<!--

Random Codes

import tensorflow as tf

# Set the seed for hash based operations in python
os.environ['PYTHONHASHSEED'] = '0'

# Set the numpy seed
np.random.seed(111)

# Disable multi-threading in tensorflow ops
session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)

# Set the random seed in tensorflow at graph level
tf.set_random_seed(111)

# Define a tensorflow session with above session configs
sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)

# Set the session in keras
K.set_session(sess)

# Make the augmentation sequence deterministic
aug.seed(111)
-->

```python
univ_seed=42
np.random.seed(univ_seed)
```

### **The Boston Housing Data :**

The Dataset is derived from information collected by the U.S. Census Service concerning housing in the area of Boston Mass. The Boston data frame has 506 rows and 14 columns. The <code>MEDV</code> variable is the target variable.

Columns / Variables in order:

- <code>CRIM</code> - per capita crime rate by town
- <code>ZN</code> - proportion of residential land zoned for lots over 25,000 sq.ft.
- <code>INDUS</code> - proportion of non-retail business acres per town
- <code>CHAS</code> - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
- <code>NOX</code> - nitric oxides concentration (parts per 10 million)
- <code>RM</code> - average number of rooms per dwelling
- <code>AGE</code> - proportion of owner-occupied units built prior to 1940
- <code>DIS</code> - weighted distances to five Boston employment centres
- <code>RAD</code> - index of accessibility to radial highways
- <code>TAX</code> - full-value property-tax rate per \$10,000
- <code>PTRATIO</code> - pupil-teacher ratio by town
- <code>B</code> - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
- <code>LSTAT</code> - % lower status of the population
- <code>MEDV</code> - Median value of owner-occupied homes in \$1000's

<!--References
 The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic
 prices and the demand for clean air', J. Environ. Economics & Management,
 vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics
 ...', Wiley, 1980.   N.B. Various transformations are used in the table on
 pages 244-261 of the latter.
-->

Importing the dataset :

```python
from sklearn.datasets import load_boston         # scikit learn has an inbuilt dataset library which includes the boston housing dataset
boston_dataset = load_boston()                   # the boston_dataset is a dictionary of values containing the data
df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)  # creating a dataframe of the boston_dataest
df['MEDV'] = boston_dataset.target               # adding the target variable to the dataframe
df.head(4)                                       # printing the first 4 columns of the dataframe
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0.0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1.0</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
  </tbody>
</table>
</div>

{% capture notice-text2 %}
The variable <code>boston_dataset</code> is python dictionary returned via the scikit-learn library with the following keys:

- data : The input values of the dataset.
- target : The output variable of the dataset.
- feature_names : The name of the feature variables as an array.
- DESCR : A brief description of the dataset.
- filename : Local location of the file with it's full path.

You can access each key's values using <code>boston_dataset.key_name</code> as we used to create a pandas dataframe. You can read the [official scikit learn datasets documentation](https://scikit-learn.org/stable/datasets/index.html#toy-datasets) and get to know about embedded datasets.
{% endcapture %}

<div class="notice--success">
  <h4 class="no_toc">Library Information :</h4>
  {{ notice-text2 | markdownify }}
</div>

<!--info-->

{% capture notice-text3 %}
You can also run the following code, provided you have cloned our [github repository](https://github.com/khanfarhan10/DIMENSIONALITY_REDUCTION).

<code>df= pd.read_excel("/content/DIMENSIONALITY_REDUCTION/data/Boston_Data.xlsx")</code>

Also, with a working internet connection, you can run :

<code>df= pd.read_excel("https://raw.githubusercontent.com/khanfarhan10/DIMENSIONALITY_REDUCTION/master/data/Boston_Data.xlsx")</code>

**--OR--**

<code>df= pd.read_excel("https://github.com/khanfarhan10/DIMENSIONALITY_REDUCTION/blob/master/data/Boston_Data.xlsx?raw=true")</code>
{% endcapture %}

<div class="notice--warning">
  <h4 class="no_toc">Alternatively :</h4>
  {{ notice-text3 | markdownify }}
</div>

<!--

https://stackoverflow.com/questions/55240330/how-to-read-csv-file-from-github-using-pandas

-->

{% capture notice-text4 %}
You might want to try <code>df.isnull().sum()</code> , <code>df.info()</code> , <code>df.describe()</code> to get the columnwise null values, dataframe information and row-wise description respectively. However , here the data provided is clean and free from such issues which would be needed to be processed/handled inspectionally.

{% endcapture %}

<div class="notice--danger">
  <h4 class="no_toc">Data Insights :</h4>
  {{ notice-text4 | markdownify }}
</div>

```python
df.hist(bins=30,figsize=(20,10),grid=False,color="crimson"); #cyan , magenta , crimson,  #column="MEDV",,color='lime'
plt.savefig("Boston Dataset Frequency Distribution of Numerical Data.png",dpi=600)
```

![Boston Dataset Frequency Distribution of Numerical Data.png](/assets/images/DIM_REDN/Boston Dataset Frequency Distribution of Numerical Data.png)

{% capture notice-text5 %}
For more color palettes visit : [Matplotlib Named Colours](https://matplotlib.org/examples/color/named_colors.html).

Almost all the visualizations used in this chapter from Pandas and Seaborn can be saved to high quality pictures using <code>plt.savefig("Fig_name.png",dpi=600)</code>

{% endcapture %}

<div class="notice--success">
  <h4 class="no_toc">Data Visualization Tips :</h4>
  {{ notice-text5 | markdownify }}
</div>

### Pearson Coefficient Correlation Matrix

The **Pearson Correlation Coefficient** (also known as the Pearson R Test) is a very useful statistical formulae that measures the strength between features and relations.

$$
r_{xy}=\frac{N \Sigma x y-(\Sigma x)(\Sigma y)}{\sqrt{\left[N \Sigma x^{2}-(\Sigma x)^{2}\right]\left[N \Sigma y^{2}-(\Sigma y)^{2}\right]}}
$$

where  
$$r_{xy}$$ = Pearson's Correlation Coefficient between variables x & y  
$$N$$ = number of pairs of x & y variables in the data  
$$\Sigma x y$$ = sum of products between x & y variables  
$$\Sigma x $$ = sum of x values  
$$\Sigma y $$ = sum of y values  
$$\Sigma x^{2}$$ = sum of squared x values  
$$\Sigma y^{2}$$ = sum of squared y values

For all feaure variables $$f_{i}$$ $$\epsilon$$ $$C$$ arranged in any order , with $$n(C) = N$$  
The Correlation Coefficient Matrix is $$M_{N \times N}$$ , where  
$$M_{ij}$$ = $$r_{ij}$$ , $$i,j$$ $$\epsilon$$ $$C$$

We will now use Pandas to get the correlation matrix and plot a heatmap using Seaborn.

<!--
r=\frac{\sum\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum\left(x_{i}-\bar{x}\right)^{2} \sum\left(y_{i}-\bar{y}\right)^{2}}}

$r=\frac{\sum\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum\left(x_{i}-\bar{x}\right)^{2} \sum\left(y_{i}-\bar{y}\right)^{2}}}$

$$
r=\frac{\sum\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum\left(x_{i}-\bar{x}\right)^{2} \sum\left(y_{i}-\bar{y}\right)^{2}}}
$$

\begin{equation}
r=\frac{\sum\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum\left(x_{i}-\bar{x}\right)^{2} \sum\left(y_{i}-\bar{y}\right)^{2}}}
\end{equation}

$$
r=\frac{\sum\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum\left(x_{i}-\bar{x}\right)^{2} \sum\left(y_{i}-\bar{y}\right)^{2}}}
$$


https://study.com/academy/lesson/pearson-correlation-coefficient-formula-example-significance.html


The Pearson correlation coefficient is a very helpful statistical formula that measures the strength between variables and relationships. In the field of statistics, this formula is often referred to as the Pearson R test. When conducting a statistical test between two variables, it is a good idea to conduct a Pearson correlation coefficient value to determine just how strong that relationship is between those two variables.

https://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/

where
r	=	correlation coefficient
$x_{i}$	=	values of the x-variable in a sample
$\bar{x}$	=	mean of the values of the x-variable
$y_{i}$	=	values of the y-variable in a sample
$\bar{y}$	=	mean of the values of the y-variable

-->

```python
correlation_matrix = df.corr().round(2) #default method is : â€˜pearsonâ€™, also available : â€˜kendallâ€™, â€˜spearmanâ€™ correlation coefficient
plt.figure(figsize=(20,10)) #set the figure size to display
sns.heatmap(data=correlation_matrix,cmap="inferno", annot=True)  # annot = True to print the values inside the squares
plt.savefig("Correlation_Data.png",dpi=600)
```

![Correlation_Data.png](/assets/images/DIM_REDN/Correlation_Data.png)

### **ML Model with reduced dataset**

The correlation coefficient ranges from -1 to 1. If the value is close to 1, it means that there is a strong positive correlation between the two variables. When it is close to -1, the variables have a strong negative correlation.

### **Observations:**

To fit a linear regression model, we select those features which have a high correlation with our target variable MEDV. By looking at the correlation matrix we can see that RM has a strong positive correlation with MEDV (0.7) where as LSTAT has a high negative correlation with MEDV(-0.74). An important point in selecting features for a linear regression model is to check for multi-co-linearity. The features RAD, TAX have a correlation of 0.91. These feature pairs are strongly correlated to each other. We should not select both these features together for training the model. Check this for an explanation. Same goes for the features DIS and AGE which have a correlation of -0.75.

Based on the above observations we will use RM and LSTAT as our features. Using a scatter plot letâ€™s see how these features vary with MEDV.

```python
plt.figure(figsize=(25,5))

features = ['LSTAT', 'RM']
target = df['MEDV']

for i, col in enumerate(features):
  plt.subplot(1, len(features), i+1)
  x = df[col]
  y = target
  plt.scatter(x, y, marker='o')
  plt.title("Variation of MEDV w.r.t. LSTAT and RM")
  plt.xlabel(col)
  plt.ylabel('MEDV')
```

Extract the Input Feature Variables in $$X$$ & Output Target Variable in $$y$$.

```python
X = df.drop(columns=['MEDV']) # select all columns except the target
y = df['MEDV']                # select the target column median value of homes
print('Shape of X : {} , Shape of y : {}'.format(X.shape,y.shape)) #get the shapes of the Input and Output Variables
```

    Shape of X : (506, 13) , Shape of y : (506,)

### Feature Scaling

Feature scaling/standardization helps machine learning models converge faster to a global optima by transforming the data to have zero mean and a unit variance of 1 hence making the data unitless.

$$
x^{\prime}=\frac{x-\mu}{\sigma}
$$

where  
$$x$$= Input Feature Variable  
$$x^{\prime}$$ = Standardized Value of $x$  
$$\mu$$= Mean value of $x$ ($\bar{x}$)  
$$\sigma=\sqrt{\frac{\sum\left(x_{i}-\mu\right)^{2}}{N}}$$ (Standard Deviation)  
$$x_{i}$$ = Each value in $$x$$  
$$N$$ = No. of Observations in $$x$$ (Size of $$x$$)

```python
from sklearn.preprocessing import StandardScaler

scaler_X = StandardScaler()
X_norm= scaler_X.fit_transform(X)

scaler_y = StandardScaler()
y_norm= scaler_y.fit_transform(X)
```

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)
print('Shape of X_train : {} , Shape of X_test : {}'.format(X_train.shape,X_test.shape))
print('Shape of y_train : {} , Shape of y_test : {}'.format(y_train.shape,X_test.shape))
```

    Shape of X_train : (354, 13) , Shape of X_test : (152, 13)
    Shape of y_train : (354,) , Shape of y_test : (152, 13)

---

## End for now.

Alternatively you can also run the following code, provided you have cloned our repo. link

```
df= pd.read_excel("DIMENSIONALITY_REDUCTION/DATASETS/BOSTON.xlsx")
df= pd.read_excel("DIMENSIONALITY_REDUCTION/data/Boston_Data.xlsx")
```

Pro Tip/ Data Insights : You might want to try <code>df.isnull().sum()</code> , <code>df.info()</code> , <code>df.describe()</code> to get the columnwise null values, dataframe information and row-wise description respectively. However , here the data provided is clean and free from such issues which would be needed to be processed/handled inspectionally.

For more color palettes visit : link.

```
df.hist(bins=30,figsize=(20,10),grid=False,color="crimson"); #cyan , magenta , crimson,  #column="MEDV",,color='lime'
plt.savefig("Boston Dataset Frequency Distribution of Numerical Data.png",dpi=600)
```

![png](DSDA_Dimensionality_Reduction_Algorithms_Final_v2_files/DSDA_Dimensionality_Reduction_Algorithms_Final_v2_19_0.png)

```
correlation_matrix = df.corr().round(2)
# annot = True to print the values inside the square
plt.figure(figsize=(20,10))
sns.heatmap(data=correlation_matrix,cmap="inferno", annot=True)
plt.savefig("Correlation_Data.png",dpi=600)
```

![png](DSDA_Dimensionality_Reduction_Algorithms_Final_v2_files/DSDA_Dimensionality_Reduction_Algorithms_Final_v2_20_0.png)

```
df.columns
```

    Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',
           'PTRATIO', 'B', 'LSTAT', 'MEDV'],
          dtype='object')

```
X = df.drop(columns=['MEDV'])
y = df['MEDV']

print('Shape of X : {} , Shape of y : {}'.format(X.shape,y.shape))
```

    Shape of X : (506, 13) , Shape of y : (506,)

```
# Normalization of the Data
from sklearn.preprocessing import StandardScaler

scaler_X = StandardScaler()
X_norm= scaler_X.fit_transform(X)

scaler_y = StandardScaler()
y_norm= scaler_y.fit_transform(X)


```

```
"abc {} xyz {}".format(1,2)
```

    'abc 1 xyz 2'

```
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)
print('Shape of X_train : {} , Shape of X_test : {}'.format(X_train.shape,X_test.shape))
print('Shape of y_train : {} , Shape of y_test : {}'.format(y_train.shape,X_test.shape))
```

    Shape of X_train : (354, 13) , Shape of X_test : (152, 13)
    Shape of y_train : (354,) , Shape of y_test : (152, 13)

```
from sklearn.linear_model import LinearRegression

lin_model = LinearRegression()
lin_model.fit(X_train, y_train)
```

    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)

```
from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score

accLR=lin_model.score(X_test,y_test)
print(accLR)
```

    0.7112260057484874

```
lin_model.coef_ #theta 1-13
```

    array([-1.33470103e-01,  3.58089136e-02,  4.95226452e-02,  3.11983512e+00,
           -1.54170609e+01,  4.05719923e+00, -1.08208352e-02, -1.38599824e+00,
            2.42727340e-01, -8.70223437e-03, -9.10685208e-01,  1.17941159e-02,
           -5.47113313e-01])

```
lin_model.coef_.shape
```

    (13,)

```
lin_model.intercept_ #theta 0
```

    31.631084035691632

```
df.columns
```

    Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',
           'PTRATIO', 'B', 'LSTAT', 'MEDV'],
          dtype='object')

```
# Coefficient & Intercept :
def merge_list_to_dict(test_keys,test_values):
  """Uses dictionary comprehension to create a dict from 2 lists"""
  merged_dict = {test_keys[i]: test_values[i] for i in range(len(test_keys))}
  return merged_dict

def get_params(model_intercept,model_coefficients,data_cols):
  """Returns a dataframe of organised values for model parameters output with colums of the original dataframe"""
  res_theta=[model_intercept ]+ list(model_coefficients)
  res_y=['INTERCEPT']+list(data_cols)
  dict_res=merge_list_to_dict(res_y,res_theta)
  data=[dict_res]
  results=pd.DataFrame(data)
  return results

res=get_params(lin_model.intercept_,lin_model.coef_,X.columns)
res.head()
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>INTERCEPT</th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>31.631084</td>
      <td>-0.13347</td>
      <td>0.035809</td>
      <td>0.049523</td>
      <td>3.119835</td>
      <td>-15.417061</td>
      <td>4.057199</td>
      <td>-0.010821</td>
      <td>-1.385998</td>
      <td>0.242727</td>
      <td>-0.008702</td>
      <td>-0.910685</td>
      <td>0.011794</td>
      <td>-0.547113</td>
    </tr>
  </tbody>
</table>
</div>

```
help(get_params)
```

    Help on function get_params in module __main__:

    get_params(model_intercept, model_coefficients, data_cols)
        Returns a dataframe of organised values for model parameters output with colums of the original dataframe

```
#demonstrating the purpose of the docstring
def tanni():
  """Meow meow meow"""
  return 1
help(tanni)
```

    Help on function tanni in module __main__:

    tanni()
        Meow meow meow

```


```

```

```

```

```

```

```

```

```

```
preds=lin_model.predict(X_test)

preds.mean()
```

    21.33015845218848

```
y_test.mean()
```

    21.407894736842103

```
temp=[]
for a,b in zip(preds,y_test):
  temp.append(abs(a-b))
temp=np.array(temp)/len(temp)
1-temp.mean()
```

    0.9791926982140957

```

```

```

```

```
df.columns
```

    Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',
           'PTRATIO', 'B', 'LSTAT', 'MEDV'],
          dtype='object')

```

```

```

```

```

```

```
round(2.7346374,2)
```

    2.73

```
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def getevaluation(model, X_subset,y_subset,subset_type="Train",round_scores=None):
  y_subset_predict = model.predict(X_subset)
  rmse = (np.sqrt(mean_squared_error(y_subset, y_subset_predict)))
  r2 = r2_score(y_subset, y_subset_predict)
  mae=mean_absolute_error(y_subset, y_subset_predict)
  if round_scores!=None:
    rmse=round(rmse,round_scores)
    r2=round(r2,round_scores)
    mae=round(mae,round_scores)

  print("Model Performance for {} subset :: RMSE: {} | R2 score: {} | MAE: {}".format(subset_type,rmse,r2,mae))

getevaluation(model=lin_model,X_subset=X_train,y_subset=y_train,subset_type="Train",round_scores=2)
getevaluation(model=lin_model,X_subset=X_test,y_subset=y_test,subset_type="Test ",round_scores=2)
```

    Model Performance for Train subset :: RMSE: 4.75 | R2 score: 0.74 | MAE: 3.36
    Model Performance for Test  subset :: RMSE: 4.64 | R2 score: 0.71 | MAE: 3.16

```

```

```

```

```

```

```

```

```

```

```

```

```

```

```
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# model evaluation for training set
y_train_predict = lin_model.predict(X_train)



rmse = (np.sqrt(mean_squared_error(y_train, y_train_predict)))
r2 = r2_score(y_train, y_train_predict)

print("The model performance for training set")
print("--------------------------------------")
print('RMSE is {}'.format(rmse))
print('R2 score is {}'.format(r2))
print("\n")

# model evaluation for testing set
y_test_predict = lin_model.predict(X_test)
rmse = (np.sqrt(mean_squared_error(y_test, y_test_predict)))
r2 = r2_score(y_test, y_test_predict)

print("The model performance for testing set")
print("--------------------------------------")
print('RMSE is {}'.format(rmse))
print('R2 score is {}'.format(r2))

```

    The model performance for training set
    --------------------------------------
    RMSE is 4.748208239685937
    R2 score is 0.7434997532004697


    The model performance for testing set
    --------------------------------------
    RMSE is 4.638689926172867
    R2 score is 0.7112260057484874

## Metrics Report :

table

# ML Model with reduced dataset

The correlation coefficient ranges from -1 to 1. If the value is close to 1, it means that there is a strong positive correlation between the two variables. When it is close to -1, the variables have a strong negative correlation.

## Observations:

To fit a linear regression model, we select those features which have a high correlation with our target variable MEDV. By looking at the correlation matrix we can see that RM has a strong positive correlation with MEDV (0.7) where as LSTAT has a high negative correlation with MEDV(-0.74).
An important point in selecting features for a linear regression model is to check for multi-co-linearity. The features RAD, TAX have a correlation of 0.91. These feature pairs are strongly correlated to each other. We should not select both these features together for training the model. Check this for an explanation. Same goes for the features DIS and AGE which have a correlation of -0.75.
Based on the above observations we will use RM and LSTAT as our features. Using a scatter plot letâ€™s see how these features vary with MEDV.

```
df.columns
```

    Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',
           'PTRATIO', 'B', 'LSTAT', 'MEDV'],
          dtype='object')

```
correlation_matrix = df.corr().round(2)

correlation_matrix
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>CRIM</th>
      <td>1.00</td>
      <td>-0.20</td>
      <td>0.41</td>
      <td>-0.06</td>
      <td>0.42</td>
      <td>-0.22</td>
      <td>0.35</td>
      <td>-0.38</td>
      <td>0.63</td>
      <td>0.58</td>
      <td>0.29</td>
      <td>-0.39</td>
      <td>0.46</td>
      <td>-0.39</td>
    </tr>
    <tr>
      <th>ZN</th>
      <td>-0.20</td>
      <td>1.00</td>
      <td>-0.53</td>
      <td>-0.04</td>
      <td>-0.52</td>
      <td>0.31</td>
      <td>-0.57</td>
      <td>0.66</td>
      <td>-0.31</td>
      <td>-0.31</td>
      <td>-0.39</td>
      <td>0.18</td>
      <td>-0.41</td>
      <td>0.36</td>
    </tr>
    <tr>
      <th>INDUS</th>
      <td>0.41</td>
      <td>-0.53</td>
      <td>1.00</td>
      <td>0.06</td>
      <td>0.76</td>
      <td>-0.39</td>
      <td>0.64</td>
      <td>-0.71</td>
      <td>0.60</td>
      <td>0.72</td>
      <td>0.38</td>
      <td>-0.36</td>
      <td>0.60</td>
      <td>-0.48</td>
    </tr>
    <tr>
      <th>CHAS</th>
      <td>-0.06</td>
      <td>-0.04</td>
      <td>0.06</td>
      <td>1.00</td>
      <td>0.09</td>
      <td>0.09</td>
      <td>0.09</td>
      <td>-0.10</td>
      <td>-0.01</td>
      <td>-0.04</td>
      <td>-0.12</td>
      <td>0.05</td>
      <td>-0.05</td>
      <td>0.18</td>
    </tr>
    <tr>
      <th>NOX</th>
      <td>0.42</td>
      <td>-0.52</td>
      <td>0.76</td>
      <td>0.09</td>
      <td>1.00</td>
      <td>-0.30</td>
      <td>0.73</td>
      <td>-0.77</td>
      <td>0.61</td>
      <td>0.67</td>
      <td>0.19</td>
      <td>-0.38</td>
      <td>0.59</td>
      <td>-0.43</td>
    </tr>
    <tr>
      <th>RM</th>
      <td>-0.22</td>
      <td>0.31</td>
      <td>-0.39</td>
      <td>0.09</td>
      <td>-0.30</td>
      <td>1.00</td>
      <td>-0.24</td>
      <td>0.21</td>
      <td>-0.21</td>
      <td>-0.29</td>
      <td>-0.36</td>
      <td>0.13</td>
      <td>-0.61</td>
      <td>0.70</td>
    </tr>
    <tr>
      <th>AGE</th>
      <td>0.35</td>
      <td>-0.57</td>
      <td>0.64</td>
      <td>0.09</td>
      <td>0.73</td>
      <td>-0.24</td>
      <td>1.00</td>
      <td>-0.75</td>
      <td>0.46</td>
      <td>0.51</td>
      <td>0.26</td>
      <td>-0.27</td>
      <td>0.60</td>
      <td>-0.38</td>
    </tr>
    <tr>
      <th>DIS</th>
      <td>-0.38</td>
      <td>0.66</td>
      <td>-0.71</td>
      <td>-0.10</td>
      <td>-0.77</td>
      <td>0.21</td>
      <td>-0.75</td>
      <td>1.00</td>
      <td>-0.49</td>
      <td>-0.53</td>
      <td>-0.23</td>
      <td>0.29</td>
      <td>-0.50</td>
      <td>0.25</td>
    </tr>
    <tr>
      <th>RAD</th>
      <td>0.63</td>
      <td>-0.31</td>
      <td>0.60</td>
      <td>-0.01</td>
      <td>0.61</td>
      <td>-0.21</td>
      <td>0.46</td>
      <td>-0.49</td>
      <td>1.00</td>
      <td>0.91</td>
      <td>0.46</td>
      <td>-0.44</td>
      <td>0.49</td>
      <td>-0.38</td>
    </tr>
    <tr>
      <th>TAX</th>
      <td>0.58</td>
      <td>-0.31</td>
      <td>0.72</td>
      <td>-0.04</td>
      <td>0.67</td>
      <td>-0.29</td>
      <td>0.51</td>
      <td>-0.53</td>
      <td>0.91</td>
      <td>1.00</td>
      <td>0.46</td>
      <td>-0.44</td>
      <td>0.54</td>
      <td>-0.47</td>
    </tr>
    <tr>
      <th>PTRATIO</th>
      <td>0.29</td>
      <td>-0.39</td>
      <td>0.38</td>
      <td>-0.12</td>
      <td>0.19</td>
      <td>-0.36</td>
      <td>0.26</td>
      <td>-0.23</td>
      <td>0.46</td>
      <td>0.46</td>
      <td>1.00</td>
      <td>-0.18</td>
      <td>0.37</td>
      <td>-0.51</td>
    </tr>
    <tr>
      <th>B</th>
      <td>-0.39</td>
      <td>0.18</td>
      <td>-0.36</td>
      <td>0.05</td>
      <td>-0.38</td>
      <td>0.13</td>
      <td>-0.27</td>
      <td>0.29</td>
      <td>-0.44</td>
      <td>-0.44</td>
      <td>-0.18</td>
      <td>1.00</td>
      <td>-0.37</td>
      <td>0.33</td>
    </tr>
    <tr>
      <th>LSTAT</th>
      <td>0.46</td>
      <td>-0.41</td>
      <td>0.60</td>
      <td>-0.05</td>
      <td>0.59</td>
      <td>-0.61</td>
      <td>0.60</td>
      <td>-0.50</td>
      <td>0.49</td>
      <td>0.54</td>
      <td>0.37</td>
      <td>-0.37</td>
      <td>1.00</td>
      <td>-0.74</td>
    </tr>
    <tr>
      <th>MEDV</th>
      <td>-0.39</td>
      <td>0.36</td>
      <td>-0.48</td>
      <td>0.18</td>
      <td>-0.43</td>
      <td>0.70</td>
      <td>-0.38</td>
      <td>0.25</td>
      <td>-0.38</td>
      <td>-0.47</td>
      <td>-0.51</td>
      <td>0.33</td>
      <td>-0.74</td>
      <td>1.00</td>
    </tr>
  </tbody>
</table>
</div>

```

```

```
to_keep=[""]
```

```

```

```

```

```

```

```

```

```

```

```

```

```
# Lasso/Ridge Regression

```

```

```

```

```

```

```

```

```

```

```

```

```

```
sns.pairplot(df);
```

    <seaborn.axisgrid.PairGrid at 0x7f7d53b7c828>

![png](DSDA_Dimensionality_Reduction_Algorithms_Final_v2_files/DSDA_Dimensionality_Reduction_Algorithms_Final_v2_77_1.png)

```

```

```

```

```

```

In this story, we applied the concepts of linear regression on the Boston housing dataset. I would recommend to try out other datasets as well.
Here are a few places you can look for data
https://www.kaggle.com/datasets
https://toolbox.google.com/datasetsearch
https://archive.ics.uci.edu/ml/datasets.html

## References

1. abc
2. bhshsh
3. jhgde
