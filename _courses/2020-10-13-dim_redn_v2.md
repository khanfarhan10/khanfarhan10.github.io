---
title: "High Dimensionality Dataset Reduction Methodologies in Applied Machine Learning"
author_profile: false
layout: single
classes: wide
toc: true
toc_sticky: false
tags:
  - Dimensionality Reduction Methodologies
  - Machine Learning
  - High Dimensionality Data Handling
date: 2020-10-14 08:15:00 +0530
categories: courses
header:
  teaser: "/assets/images/DIM_REDN/DIM_RED_VIZ.png"
excerpt: "Taylor and Francis Book Publication (Routledge) : DIMENSIONALITY REDUCTION ALGORITHMS IN APPLIED MACHINE LEARNING"
description: "Taylor and Francis Book Publication (Routledge) : DIMENSIONALITY REDUCTION ALGORITHMS IN APPLIED MACHINE LEARNING"
og_image: "/assets/images/DIM_REDN/DIM_RED_VIZ.png"
use_math: true
mathjax: true
sidebar:
  - title: "High Dimensionality Dataset Reduction Methodologies in Applied Machine Learning"
    image: "/assets/images/DIM_REDN/DIM_RED_VIZ.png"
    image_alt: "Dimensionality Projection"
    text: "Taylor and Francis Book Publication (Routledge) : DIMENSIONALITY REDUCTION ALGORITHMS IN APPLIED MACHINE LEARNING"
    nav: DSDA_docs
---

**Farhan Hai Khan<sup>a</sup>, Tannistha Pal<sup>b</sup>**
{: .text-center}
**a. Department of Electrical Engineering, Institute of Engineering & Management, Kolkata, India, njrfarhandasilva10@gmail.com**
{: .text-center}
**b. Department of Electronics and Communication Engineering, Institute of Engineering & Management, Kolkata, India, paltannistha@gmail.com**
{: .text-center}

<!--
Useful Minimal Mistakes Theme Links:

https://mmistakes.github.io/minimal-mistakes/docs/utility-classes/

https://mmistakes.github.io/minimal-mistakes/docs/helpers/#gallery

https://mmistakes.github.io/minimal-mistakes/docs/layouts/

-->

## **Abstract**

A common problem faced while handling multi-featured datasets is the high amount of dimensionality that they often consist of, leading to barriers in generalized hands-on Machine Learning. These datasets also give a drastic impact on the performance of Machine Learning algorithms, being memory inefficient and frequently leading to model overfitting. It often becomes difficult to visualize or gain insightful knowledge on the data features such as presence of outliers.

This chapter will help data analysts reduce data dimensionality using various methodologies such as:<br>

1. Feature Selection using Covariance Matrix
2. Principal Component Analysis (PCA)
3. t-distributed Stochastic Neighbour Embedding (t-SNE)

Under applications of Dimensionality Reduction Algorithms with Visualizations, firstly, we introduce the Boston Housing Dataset and use the Correlation Matrix to apply Feature Selection on the strongly correlated data and perform Simple Linear Regression over the new features.Then we use UCI Breast Cancer Dataset to perform PCA Analysis with Support Vector Machine Classification (SVM). Lastly, we apply t-SNE to MNIST Handwritten Digits Dataset and use k-Nearest Neighbours (kNNs) clustering for classification.

Finally, we explore the benefits of using Dimensionality Reduction Methods and provide a comprehensive overview of reduction in storage space, efficient models,feature selection guidelines ,redundant data removal and outlier analysis.

**_Keywords : Dimensionality Reduction, Feature Selection, Covariance Matrix, PCA , t-SNE_**

<!--
## **Table of Contents**

1. Problems faced with Multi-Dimensional Datasets
   1. Data Intuition
   2. Data Visualization Constraints
   3. Outlier Detection
2. Dimensionality Reduction Algorithms with Visualizations
   1. Feature Selection using Covariance Matrix
   2. Principal Component Analysis (PCA)
   3. t-distributed Stochastic Neighbour Embedding (t-SNE)
3. Benefits of Dimensionality Reduction
   1. Storage Space Reduction
   2. Computation Time Optimization
   3. Redundant Feature Removal
   4. Incorrect Data Removal
-->

## 1. Problems faced with High Dimensionality Data : An Introduction

<!--References
https://thenewstack.io/3-new-techniques-for-data-dimensionality-reduction-in-machine-learning/

https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/#:~:text=Dimensionality%20reduction%20refers%20to%20techniques%20for%20reducing%20the%20number%20of,%E2%80%9Cessence%E2%80%9D%20of%20the%20data.

https://medium.com/@cxu24/why-dimensionality-reduction-is-important-dd60b5611543
-->

<!--
### 1.1 **_An Introduction on High Dimensional Data_**
-->

<!--
<i><blockquote> "...dimensionality reduction yields a more compact, more easily interpretable representation of the target concept, focusing the userâ€™s attention on the most relevant variables."</blockquote></i>

-Page 289, Data Mining: Practical Machine Learning Tools and Techniques, 4th edition, 2016.

https://amzn.to/2tlRP9V
-->

<!--
<p align='right'> Your Text </p>
<p style='text-align: right;'> Your Text </p>
<style>body {text-align: right}</style>
<div style="text-align: right"> your-text-here </div>
<p style="text-align:right">This is some text in a paragraph.</p>
-->

<blockquote> "Dimensionality Reduction leads to a comprehensive, precise & compressed depiction of the target output variables, by reducing redundant input variables." 
<p align='right'><b>- Farhan Khan & Tannistha Pal.</b></p>
</blockquote>

_In the field of artificial intelligence, data explosion has created a plethora of input data & features to be fed into machine learning algorithms. Since most of the real-world data is multi-dimensional in nature, data scientists & data analysts require the core concepts of dimensionality reduction mechanisms for better :_<br/><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_(i) Data Intuition : Visualization, Outlier Detection & Noise Reduction_<br/><br/>
&nbsp;&nbsp;&nbsp;&nbsp;_(ii) Performance Efficiency : Faster Training Intervals & Reduced Computational Processing Time_<br/><br/>
&nbsp;&nbsp;&nbsp;&nbsp;_(iii) Generalization : Prevents Overfitting (High Variance & Low Bias)_<br/><br/>
_This chapter introduces the practical working implementation of these reduction algorithms in applied machine learning._

<!--
<p>Anything you'd like to mention goes here: <blockquote>"Insert actual quote here."</blockquote> - Mr. Name</p>
-->
<!--4 nbsp - non breaking space is a tab &nbsp;&nbsp;&nbsp;&nbsp;-->

<i>Multiple features make it difficult to obtain valuable insights into data, as the visualization plots obtained can be 3-Dimensional at most. Due to this limitation, dependent properties/operations such as Outlier Detection and Noise Removal become more and more non-intuitive to perform on these humongous datasets. Therefore, applying dimensionality reduction helps in identifying these properties more effortlessly.</i>

<i>Due to this reduced/compressed form of data, faster mathematical operations such as Scaling, Classification, Regression can be performed. Also, the data is more cleansed and this further solves the issues of overfitting a model.</i>

<i>
Dimensionality Reduction can be broadly classified into :<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(i) Feature Selection Techniques : Feature Selection attempts to train the machine learning model by selectively choosing a subset of the original feature set based on some criteria. Hence, redundant and obsolete characteristics could be eliminated without much information loss. Examples - Correlation Matrix Thresholding & Chi Squared Test Selection.<br/>
&nbsp;&nbsp;&nbsp;&nbsp;(ii) Feature Extraction/Projection Techniques : This method projects the original input features from the high dimensional space by summarizing most statistics and removing redundant data / manipulating to create new relevant output features with reduced dimensionality (fewer dimensional space). Examples - Principle Component Analysis (PCA) , Linear Discriminant Analysis (LDA), t-distributed Stochastic Neighbour Embedding(t-SNE) & Isometric Mapping (IsoMap).<br/><br/></i>

_However, we have limited our discussion to Correlation Matrices, PCA & t-SNE only, as covering all such techniques is beyond the scope of this book chapter._

<!--
TODO

3+ dims hard to visualize
outlier noise follow
 del unnecessary rows
Std scaler less time, lesss size,

Quick Tip
SEEDS
-->

## 2. Dimensionality Reduction Algorithms with Visualizations

### 2.1 **_Feature Selection using Covariance Matrix_**

**Objective :** Introduce Boston Housing Dataset and use the obtained Correlation Matrix to apply Feature Selection on the strongly positive correlated data and perform Regression over the selective features.

<!-- References
https://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155

https://www.geeksforgeeks.org/ml-boston-housing-kaggle-challenge-with-linear-regression/

https://towardsdatascience.com/polynomial-regression-bbe8b9d97491

Ridge & Lasso ??? -->

<!--Alert message.-->

#### 2.1.1 **Importing the Modules**

We will need 3 datasets for this chapter, each of which have been documented on our [github repository](https://github.com/khanfarhan10/DIMENSIONALITY_REDUCTION).
Hence we will create a local copy (clone) of that repo here.

<!--You can read more about git and github here.??-->

```python
!git clone https://github.com/khanfarhan10/DIMENSIONALITY_REDUCTION.git
```

    Cloning into 'DIMENSIONALITY_REDUCTION'...
    remote: Enumerating objects: 14, done.[K
    remote: Counting objects: 100% (14/14), done.[K
    remote: Compressing objects: 100% (12/12), done.[K
    remote: Total 14 (delta 1), reused 0 (delta 0), pack-reused 0[K
    Unpacking objects: 100% (14/14), done.

Firstly we will import all the necessary libraries that we will be requiring for Dataset Reductions.

```python
import numpy as np               # Mathematical Functions , Linear Algebra, Matrix Operations
import pandas as pd              # Data Manipulations,  Data Analysis/Storing/Preparation
import matplotlib.pyplot as plt  # Simple Data Visualization , Basic Plotting Utilities
plt.style.use("dark_background") #just a preference of the authors, adds visual attractiveness
import seaborn as sns            # Advanced Data Visualization, High Level Figures Interfacing
%matplotlib inline
# used for Jupyter Notebook Plotting
#%matplotlib notebook            # This can be used as an alternative as the plots obtained will be interactive in nature.
```

<!--info-->
<!--Alert Box-->

{% capture notice-text1 %}
In Applied Machine Learning, it is essential to make experiments reproducable and at the same time keeping weights as completely random. The Seed of a Pseudo Random Number Generator (PRNG) acheives the exact same task by initializing values with the same conditions everytime a program is executed. We have used a constant value (universal seed) of 42 throughout the course of this chapter.

More info on [PRNGs](https://www.geeksforgeeks.org/pseudo-random-number-generator-prng/) and [Seeds](https://numpy.org/doc/stable/reference/random/generator.html).
{% endcapture %}

<div class="notice--primary">
  <h4 class="no_toc">Initial Pseudo Random Number Generator Seeds:</h4>
  {{ notice-text1 | markdownify }}
</div>

<!-- original text
**Initial Pseudo Random Number Generator Seeds**

In Applied Machine Learning, it is essential to make experiments reproducable and at the same time keeping weights as completely random. The Seed of a Pseudo Random Number Generator (PRNG) acheives the exact same task by initializing values with the same conditions everytime a program is executed. We have used a constant value (universal seed) of 42 throughout the course of this chapter.

More info on [PRNGs](https://www.geeksforgeeks.org/pseudo-random-number-generator-prng/) and [Seeds](https://numpy.org/doc/stable/reference/random/generator.html).
{: .notice--primary}
-->

<!--

Random Codes

import tensorflow as tf

# Set the seed for hash based operations in python
os.environ['PYTHONHASHSEED'] = '0'

# Set the numpy seed
np.random.seed(111)

# Disable multi-threading in tensorflow ops
session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)

# Set the random seed in tensorflow at graph level
tf.set_random_seed(111)

# Define a tensorflow session with above session configs
sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)

# Set the session in keras
K.set_session(sess)

# Make the augmentation sequence deterministic
aug.seed(111)
-->

```python
univ_seed=42
np.random.seed(univ_seed)
```

#### 2.1.2 **The Boston Housing Dataset**

The Dataset is derived from information collected by the U.S. Census Service concerning housing in the area of Boston Mass. The Boston data frame has 506 rows and 14 columns. The <code>MEDV</code> variable is the target variable.

Columns / Variables in order:

- <code>CRIM</code> - per capita crime rate by town
- <code>ZN</code> - proportion of residential land zoned for lots over 25,000 sq.ft.
- <code>INDUS</code> - proportion of non-retail business acres per town
- <code>CHAS</code> - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
- <code>NOX</code> - nitric oxides concentration (parts per 10 million)
- <code>RM</code> - average number of rooms per dwelling
- <code>AGE</code> - proportion of owner-occupied units built prior to 1940
- <code>DIS</code> - weighted distances to five Boston employment centres
- <code>RAD</code> - index of accessibility to radial highways
- <code>TAX</code> - full-value property-tax rate per \$10,000
- <code>PTRATIO</code> - pupil-teacher ratio by town
- <code>B</code> - 1000(B<sub>k</sub> - 0.63)<sup>2</sup> where B<sub>k</sub> is the proportion of blacks by town
- <code>LSTAT</code> - % lower status of the population
- <code>MEDV</code> - Median value of owner-occupied homes in \$1000's

<!--References
 The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic
 prices and the demand for clean air', J. Environ. Economics & Management,
 vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics
 ...', Wiley, 1980.   N.B. Various transformations are used in the table on
 pages 244-261 of the latter.
-->

Importing the dataset :

```python
from sklearn.datasets import load_boston         # scikit learn has an inbuilt dataset library which includes the boston housing dataset
boston_dataset = load_boston()                   # the boston_dataset is a dictionary of values containing the data
df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)  # creating a dataframe of the boston_dataest
df['MEDV'] = boston_dataset.target               # adding the target variable to the dataframe
df.head(4)                                       # printing the first 4 columns of the dataframe
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0.0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1.0</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
  </tbody>
</table>
</div>

{% capture notice-text2 %}
The variable <code>boston_dataset</code> is python dictionary returned via the scikit-learn library with the following keys:

- data : The input values of the dataset.
- target : The output variable of the dataset.
- feature_names : The name of the feature variables as an array.
- DESCR : A brief description of the dataset.
- filename : Local location of the file with it's full path.

You can access each key's values using <code>boston_dataset.key_name</code> as we used to create a pandas dataframe. You can read the [official scikit learn datasets documentation](https://scikit-learn.org/stable/datasets/index.html#toy-datasets) and get to know about embedded datasets.
{% endcapture %}

<div class="notice--success">
  <h4 class="no_toc">Library Information :</h4>
  {{ notice-text2 | markdownify }}
</div>

<!--info-->

{% capture notice-text3 %}
You can also run the following code, provided you have cloned our [github repository](https://github.com/khanfarhan10/DIMENSIONALITY_REDUCTION).

<code>df= pd.read_excel("/content/DIMENSIONALITY_REDUCTION/data/Boston_Data.xlsx")</code>

Also, with a working internet connection, you can run :

<code>df= pd.read_excel("https://raw.githubusercontent.com/khanfarhan10/DIMENSIONALITY_REDUCTION/master/data/Boston_Data.xlsx")</code>

**--OR--**

<code>df= pd.read_excel("https://github.com/khanfarhan10/DIMENSIONALITY_REDUCTION/blob/master/data/Boston_Data.xlsx?raw=true")</code>
{% endcapture %}

<div class="notice--warning">
  <h4 class="no_toc">Alternatively :</h4>
  {{ notice-text3 | markdownify }}
</div>

<!--

https://stackoverflow.com/questions/55240330/how-to-read-csv-file-from-github-using-pandas

-->

{% capture notice-text4 %}
You might want to try <code>df.isnull().sum()</code> , <code>df.info()</code> , <code>df.describe()</code> to get the columnwise null values, dataframe information and row-wise description respectively. However , here the data provided is clean and free from such issues which would be needed to be processed/handled inspectionally.

{% endcapture %}

<div class="notice--danger">
  <h4 class="no_toc">Data Insights :</h4>
  {{ notice-text4 | markdownify }}
</div>

#### 2.1.3 **Perform Basic Data Visualization**

Data Visualization is the key to visual data insights and can provide useful analytics about the data. Here in the following code snippet, we will find out the distribution of each columns (feature & target) in the data.

```python
df.hist(bins=30,figsize=(20,10),grid=False,color="crimson"); # distribution of each column
```

![Boston Dataset Frequency Distribution of Numerical Data.png](/assets/images/DIM_REDN/Boston Dataset Frequency Distribution of Numerical Data.png)

{% capture notice-text5 %}
For more color palettes visit : [Matplotlib Named Colours](https://matplotlib.org/examples/color/named_colors.html).

Almost all the visualizations used in this chapter from Pandas and Seaborn can be saved to high quality pictures using <code>plt.savefig("fig_name.png",dpi=600)</code>

{% endcapture %}

<div class="notice--success">
  <h4 class="no_toc">Data Visualization Tips :</h4>
  {{ notice-text5 | markdownify }}
</div>

#### 2.1.4 **Pearson Coefficient Correlation Matrix**

The **Pearson Correlation Coefficient** (also known as the Pearson R Test) is a very useful statistical formulae that measures the strength between features and relations.
Mathematically,

$$
r_{xy}=\frac{N \Sigma x y-(\Sigma x)(\Sigma y)}{\sqrt{\left[N \Sigma x^{2}-(\Sigma x)^{2}\right]\left[N \Sigma y^{2}-(\Sigma y)^{2}\right]}}
$$

where  
$$r_{xy}$$ = Pearson's Correlation Coefficient between variables x & y  
$$N$$ = number of pairs of x & y variables in the data  
$$\Sigma x y$$ = sum of products between x & y variables  
$$\Sigma x $$ = sum of x values  
$$\Sigma y $$ = sum of y values  
$$\Sigma x^{2}$$ = sum of squared x values  
$$\Sigma y^{2}$$ = sum of squared y values

For all feaure variables $$f_{i}$$ $$\epsilon$$ $$F$$ arranged in any order , with $$n(F) = N$$  
The Correlation Coefficient Matrix is $$M_{N \times N}$$ , where  
$$M_{ij}$$ = $$r_{ij}$$ , $$i,j$$ $$\epsilon$$ $$F$$

We will now use Pandas to get the correlation matrix and plot a heatmap using Seaborn.

<!--
r=\frac{\sum\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum\left(x_{i}-\bar{x}\right)^{2} \sum\left(y_{i}-\bar{y}\right)^{2}}}

$r=\frac{\sum\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum\left(x_{i}-\bar{x}\right)^{2} \sum\left(y_{i}-\bar{y}\right)^{2}}}$

$$
r=\frac{\sum\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum\left(x_{i}-\bar{x}\right)^{2} \sum\left(y_{i}-\bar{y}\right)^{2}}}
$$

\begin{equation}
r=\frac{\sum\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum\left(x_{i}-\bar{x}\right)^{2} \sum\left(y_{i}-\bar{y}\right)^{2}}}
\end{equation}

$$
r=\frac{\sum\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum\left(x_{i}-\bar{x}\right)^{2} \sum\left(y_{i}-\bar{y}\right)^{2}}}
$$


https://study.com/academy/lesson/pearson-correlation-coefficient-formula-example-significance.html


The Pearson correlation coefficient is a very helpful statistical formula that measures the strength between variables and relationships. In the field of statistics, this formula is often referred to as the Pearson R test. When conducting a statistical test between two variables, it is a good idea to conduct a Pearson correlation coefficient value to determine just how strong that relationship is between those two variables.

https://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/

where
r	=	correlation coefficient
$x_{i}$	=	values of the x-variable in a sample
$\bar{x}$	=	mean of the values of the x-variable
$y_{i}$	=	values of the y-variable in a sample
$\bar{y}$	=	mean of the values of the y-variable

-->

```python
correlation_matrix = df.corr().round(2) #default method = â€˜pearsonâ€™, also available : â€˜kendallâ€™, â€˜spearmanâ€™ correlation coefficients
plt.figure(figsize=(20,10)) #set the figure size to display
sns.heatmap(data=correlation_matrix,cmap="inferno", annot=True)  # annot = True to print the values inside the squares
plt.savefig("Correlation_Data.png",dpi=600)
```

![Correlation_Data.png](/assets/images/DIM_REDN/Correlation_Data.png)

#### 2.1.5 **Detailed Correlation Matrix Analysis**

The correlation coefficient ranges from -1 to 1. If the value is close to 1, it means that there is a strong positive correlation between the two variables and both increase or decrease simultaneously. When it is close to -1, the relationship between the variables is negatively correlated, or as one value increases, the other decreases.

**Observations:**  
To fit a linear regression model, we select those features which have a high correlation with our target variable <code>MEDV</code>. By looking at the correlation matrix we can see that <code>RM</code> has a strong positive correlation with <code>MEDV (0.7)</code> where as <code>LSTAT (-0.74)</code> has a high negative correlation with <code>MEDV</code>. An important point in selecting features for a linear regression model is to check for **Multi Collinearity** : features that are strongly correlated to other features and are therefore redundant. The features <code>RAD</code>, <code>TAX</code> have a correlation of <code>0.91</code>. These feature pairs are strongly correlated to each other . We should not select both these features together for training the model. The same goes for the features <code>DIS</code> and <code>AGE</code> which have a correlation of <code>-0.75</code>. Except for a manual analysis of the correlation, the function below computes the strongly correlated features to the target variable <code>MEDV</code> :

```python
thres_range=(-0.7,0.7)  # provide the upper and lower limits for thresholding the strongly correlated features
target_variable="MEDV"  # provide the target variable name

def get_strong_corr(correlation_matrix,target_variable,thres_range=(-0.65,0.65)):
  """
  Get the strongly positive and strongly negatively correlated components from the provided correlation matrix.
  Assigns values inside boundary to 0 and returns non zero entries as a Pandas Series.
  correlation_matrix : The correlation matrix obtained from the data.
  target_variable    : The name of the target variable that we need to calculate the correlation for.
  thres_range        : The thresholding range for the calculation of strongly correlated data.
  """
  thres_min,thres_max=thres_range                                       # assign minimum and maximum values passed to threshold
  target_row=correlation_matrix[target_variable]                        # get the row with the target variable name
  target_row[(target_row > thres_min) & (target_row < thres_max)]=0     # assign values out of given threshold to zero
  indices_thresholded=target_row.to_numpy().nonzero()                   # remove the zero values from the filtered target row and get indices
  strong_corr=list(correlation_matrix.columns[indices_thresholded])     # extract feature names from their respective indices
  if target_variable in strong_corr: strong_corr.remove(target_variable)# correlation of target variable with itself is always 1, remove it.
  return target_row[strong_corr]                                        # return the strongly correlated features with their values

strong_corr=get_strong_corr(correlation_matrix,target_variable,thres_range)
print(strong_corr)
```

    RM       0.70
    LSTAT   -0.74
    Name: MEDV, dtype: float64

{% capture notice-text5 %}
Triple quoted strings (<code>"""String"""</code>) after a function declaration in Python account for a function's documentation and are referred to as **Docstrings**. These can be retrieved later and add the advantage of asking help towards the working of a function.  
Create Docstring :  
<code>def function_name(arguments):
"""Function Documentation""""  
</code>

Retrieve Helper Docstring :  
<code>help(function_name)</code>

For information about Docstrings visit : [Docstring Conventions](https://www.python.org/dev/peps/pep-0257/). For example you could run the following command:  
<code>help(get_strong_corr)</code>

{% endcapture %}

<div class="notice--warning">
  <h4 class="no_toc">Python Code Documentation - Docstrings :</h4>
  {{ notice-text5 | markdownify }}
</div>

Based on the above observations and discussions we will use <code>RM</code> and <code>LSTAT</code> as our features. Using a scatter plot letâ€™s see how these features vary with <code>MEDV</code>.

```python
plt.figure(figsize=(25,10))                                    # initialize the figure with a figure size

features = ['LSTAT', 'RM']                                     # features to display over the dataset

for i, col in enumerate(features):                             # loop over the features with count
    plt.subplot(1, len(features) , i+1)                        # subplotting
    x = df[col]                                                # getting the column values from the dataframe
    plt.scatter(x, target, marker='o',color="cyan")            # performing a scatterplot in matplotlib over x & target
    plt.title("Variation of "+target_variable+" w.r.t. "+col)  # setting subplot title
    plt.xlabel(col)                                            # setting the xlabels and ylabels
    plt.ylabel(target_variable)
```

![Variation of MEDV wrt variables LSTAT and RM.png](/assets/images/DIM_REDN/Variation of MEDV wrt variables LSTAT and RM.png)

#### 2.1.6 **3-Dimensional Data Visualization**

The added advantage of performing Dimensionality Reduction is that 3-D visualizations are now possible over the input features (<code>LSTAT</code> & <code>RM</code>) and the target output(<code>MEDV</code>). These visual interpretations of the data help us obtain a concise overview of the model hypothesis complexity that needs to be considered to prevent overfitting.

```python
import plotly.graph_objects as go                      # plotly provides interactive 3D plots
from plotly.graph_objs.layout.scene import XAxis, YAxis, ZAxis

df_sampled= df.sample(n = 100,random_state=univ_seed)  # use random sampling to avoid cumbersome overcrowded plots

LSTAT, RM, MEDV = df_sampled["LSTAT"], df_sampled["RM"], df_sampled["MEDV"]

# set the Plot Title , Axis Labels , Tight Layout , Theme
layout = go.Layout(
    title="Boston Dataset 3-Dimensional Visualizations",
    scene = dict( xaxis = XAxis(title='LSTAT'), yaxis = YAxis(title='RM'), zaxis = ZAxis(title='MEDV'), ),
    margin=dict(l=0, r=0, b=0, t=0),
    template="plotly_dark"
)

# create the scatter plot with required hover information text
trace_scatter= go.Scatter3d(
    x=LSTAT,
    y=RM,
    z=MEDV,
    mode='markers',
    marker=dict(
        size=12,
        color=MEDV,
        showscale=True,         # set color to an array/list of desired values
        colorscale='inferno',   # choose a colorscale: viridis
        opacity=0.8
    ),
    text= [f"LSTAT: {a}<br>RM: {b}<br>MEDV: {c}" for a,b,c in list(zip(LSTAT,RM,MEDV))],
    hoverinfo='text'
)

#get the figure using the layout on the scatter trace
fig = go.Figure(data=[trace_scatter],layout=layout)

fig.write_html("Boston_3D_Viz.html") # save the figure to html
fig.show()                           # display the figure
```

<iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="https://plotly.com/~khanfarhan10/1.embed" height="525" width="100%"></iframe>

{% capture notice-text6 %}
To save the 3D plots externally for future purposes use <code>fig.write_html("Boston_3D_Viz.html")</code> to save the interactive plots to HTML files accessible by internet browsers.
{% endcapture %}

<div class="notice--success">
  <h4 class="no_toc">Saving Interactive Plots :</h4>
  {{ notice-text6 | markdownify }}
</div>

**Conclusions based on Visual Insights :**

- Home Prices(<code>MEDV</code>) tend to decrease with the increase in <code>LSTAT</code>. The curve follows a linear - semi-quadratic equation in nature.

- Home Prices(<code>MEDV</code>) tend to increase with the increase in <code>RM</code> linearly. There are few outliers present in the dataset as clearly portrayed by the 3-D Visualization.

#### 2.1.7 **Extracting the Features and Target**

Extract the Input Feature Variables in $$X$$ & Output Target Variable in $$y$$.

```python
X = pd.DataFrame(np.c_[df['LSTAT'], df['RM']], columns = ['LSTAT', 'RM']) # concatenate LSTAT and RM columns using numpy np.c_ function
y = df['MEDV']                                                            # store the target column median value of homes (MEDV) in y
print('Dataframe Shapes : ','Shape of X : {} , Shape of y : {}'.format(X.shape,y.shape)) # print the shapes of the Input and Output Variables
X=X.to_numpy()    # Convert the Input Feature DataFrame X to a NumPy Array
y=y.to_numpy()    # Convert the Output Target DataFrame y to a NumPy Array
y=y.reshape(-1,1) # Shorthand method to reshape numpy array to single column format
print('Array Shapes : ','Shape of X : {} , Shape of y : {}'.format(X.shape,y.shape))     # print the shapes of the Input and Output Variables
```

    Dataframe Shapes :  Shape of X : (506, 2) , Shape of y : (506,)
    Array Shapes :  Shape of X : (506, 2) , Shape of y : (506, 1)

#### 2.1.8 **Feature Scaling**

Feature scaling/standardization helps machine learning models converge faster to a global optima by transforming the data to have zero mean and a unit variance of 1 hence making the data unitless.

$$
x^{\prime}=\frac{x-\mu}{\sigma}
$$

where  
$$x$$= Input Feature Variable  
$$x^{\prime}$$ = Standardized Value of $x$  
$$\mu$$= Mean value of $$x$$ ($$\bar{x}$$)  
$$\sigma=\sqrt{\frac{\sum\left(x_{i}-\mu\right)^{2}}{N}}$$ (Standard Deviation)  
$$x_{i}$$ = Each value in $$x$$  
$$N$$ = No. of Observations in $$x$$ (Size of $$x$$)

```python
from sklearn.preprocessing import StandardScaler

scaler_x = StandardScaler()
X_scaled= scaler_x.fit_transform(X)

scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y)
```

#### 2.1.9 **Create Training and Testing Dataset**

Splitting the data into Test and Train Sets:

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size = 0.3, random_state=univ_seed)
print('Shape of X_train : {} , Shape of X_test : {}'.format(X_train.shape,X_test.shape))
print('Shape of y_train : {} , Shape of y_test : {}'.format(y_train.shape,y_test.shape))
```

    Shape of X_train : (354, 2) , Shape of X_test : (152, 2)
    Shape of y_train : (354, 1) , Shape of y_test : (152, 1)

#### 2.1.10 **Training and Evaluating ML Models with Reduced Dataset**

**Multivariate Linear Regression :**

_Model Training - Ordinary Least Squares :_
Here we will attempt to fit a linear regression model which would map the input features $$x_{i}$$ (<code>LSTAT</code> & <code>RM</code>) to $y$ (<code>MEDV</code>). Hence the model hypothesis :
$$h_{\Theta}(x)=\Theta_{0} + \Theta_{1}x_{1} + \Theta_{2}x_{2}$$
where  
$$y$$ = Output Target Variable <code>MEDV</code>  
$$x_{1}$$ = Input Feature Variable <code>LSTAT</code>  
$$x_{2}$$ = Input Feature Variable <code>RM</code>  
$$\Theta$$ = Model Parameters (to obtain)

We perform Ordinary Least Squares (OLS) Regression using the scikit-learn library to obtain $$\Theta_{i}$$.

```python
from sklearn.linear_model import LinearRegression

lin_model = LinearRegression()
lin_model.fit(X_train, y_train)

points_to_round=2
theta= list(lin_model.coef_.flatten().round(points_to_round))+list(lin_model.intercept_.flatten().round(points_to_round))
print("Model Parameters Obtained : ",theta)  # merge the values of theta 1 and theta 2 (coef_) and the value of theta 0 (intercept_)
```

    Model Parameters Obtained :  [-0.52, 0.38, 0.01]

```python
def get_model_params(theta,features,target_variable):
  """Pretty Print the Features with the Model Parameters"""
  text = target_variable + " = " + str(theta[0])
  for t, x in zip ( theta[1:], features) :
    text += " + "+ str(t) + " * " + str(x)
  return text
features=['LSTAT','RM']
print(get_model_params(theta,features,target_variable))
```

    MEDV = -0.52 + 0.38 * LSTAT + 0.01 * RM

_Model Evaluation - Regression Metrics :_  
We need to calculate the following values in order to evaluate our model.

- Mean Absolute Error(MAE)
- Root Mean Squared Error (RMSE)
- R-Squared Value (coefficient of determination)

<!--
- Mean Absolute Error(MAE)

$$
\operatorname{MAE}=\frac{\sum_{i=1}^{n}\left|y_{i}-x_{i}\right|}{n}
$$

$$
\begin{array}{l}
\mathrm{MAE}=\text { mean absolute error } \\
\begin{aligned}
y_{i} &=\text { prediction } \\
x_{i} &=\text { true value } \\
n &=\text { total number of data points }
\end{aligned}
\end{array}
$$

- Root Mean Squared Error (RMSE)

$$
R M S E=\sqrt{\sum_{i=1}^{n} \frac{\left(\hat{y}_{i}-y_{i}\right)^{2}}{n}}
$$

$$\hat{y}_{i}$$ are predicted values
$$y_{i}$$ are observed values
$$n$$ is the number of observations

- R-Squared Value (coefficient of determination)

$$
R^{2}=1-\frac{R S S}{T S S}
$$

$$R^{2}=$$ coefficient of detemination
$$ R S S=$$ sum of squares of residual

$$
R S S=\sum_{i=1}^{n}\left(y_{i}-f\left(x_{i}\right)\right)^{2}
$$

$$y_{i}$$ = value of the variable to be predicted
$$f\left(x_{i}\right)$$ = prediced values of $$y_{i}$$
$$n$$ = upper limit of summation
$$T S S=$$ total sum of squares
$$\mathrm{TSS}=\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}$$
$$n$$ = number of observations
$$y_{i}$$ = value in a sample
$$\bar{y}$$ = mean value of a sample

-->

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def getevaluation(model, X_subset,y_subset,subset_type="Train",round_scores=None):
  y_subset_predict = model.predict(X_subset)
  rmse = (np.sqrt(mean_squared_error(y_subset, y_subset_predict)))
  r2 = r2_score(y_subset, y_subset_predict)
  mae=mean_absolute_error(y_subset, y_subset_predict)
  if round_scores!=None:
    rmse=round(rmse,round_scores)
    r2=round(r2,round_scores)
    mae=round(mae,round_scores)

  print("Model Performance for {} subset :: RMSE: {} | R2 score: {} | MAE: {}".format(subset_type,rmse,r2,mae))

getevaluation(model=lin_model,X_subset=X_train,y_subset=y_train,subset_type="Train",round_scores=2)
getevaluation(model=lin_model,X_subset=X_test,y_subset=y_test,subset_type="Test ",round_scores=2)
```

    Model Performance for Train subset :: RMSE: 0.6 | R2 score: 0.65 | MAE: 0.43
    Model Performance for Test  subset :: RMSE: 0.59 | R2 score: 0.6 | MAE: 0.44

<!--
_Model Visualization - Regression Line Plot compared to Original Data Points :_
Here we will visualize the regression line obtained by the model in a 3-Dimensional Vector Space and compare it with the Original Data Points.
-->

#### 2.1.11 **Limitations of the Correlation Matrix Analysis**

Correlation Coefficients are a vital parameter when applying Linear Regression on your Datasets. However it is limited as :

- Only **LINEAR RELATIONSHIPS** are being considered as candidates for mapping of the target to the features. However, most mappings are non-linear in nature.
- Ordinary Least Squares (OLS) Regression is **SUSCEPTABLE TO OUTLIERS** and may learn an inaccurate hypothesis from the noisy data.
- There may be non-linear variables other than the ones chosen with Pearson Coefficient Correlation Thresholding, which have been discarded, but do **PARTIALLY INFLUENCE** the output variable.
- A strong correlation assumes a direct change in the input variable would reflect back immediately into the output variable, but there exist some variables that are **SELECTIVELY INDEPENDENT** in nature yet they provide a suitably high value of the correlation coefficient.

## **Conclusion**

In this chapter, we applied the concepts of dimensionality reduction in applied machine learning on various datasets. The authors recommend to try out other datasets as well to practice & get a firm understanding of the algorithms used in this chapter for reducing high dimensionality datasets.
Here are a few places you might look for data :

- [Kaggle Datasets](https://www.kaggle.com/datasets)
- [Google Dataset Search](https://datasetsearch.research.google.com/)
- [Github Datasets](https://github.com/awesomedata/awesome-public-datasets#machinelearning)
- [UCI Machine Learning Datasets](https://archive.ics.uci.edu/ml/datasets.html)
- [Socrata Finding Open Data](https://dev.socrata.com/data/)
